1
00:00:00,000 --> 00:00:05,500


2
00:00:05,500 --> 00:00:06,750
Hey, everyone.

3
00:00:06,750 --> 00:00:11,160
Welcome to the first and
introductory lecture for CS 25,

4
00:00:11,160 --> 00:00:12,870
Transformers United.

5
00:00:12,870 --> 00:00:15,870
So CS 25 was a class
that the three of us

6
00:00:15,870 --> 00:00:19,560
created and taught at
Stanford in the fall of 2021,

7
00:00:19,560 --> 00:00:22,470
and the subject of
the class is not

8
00:00:22,470 --> 00:00:23,760
as the picture might suggest.

9
00:00:23,760 --> 00:00:27,400
It's not about robots that
can transform into cars.

10
00:00:27,400 --> 00:00:30,060
It's about deep learning
models and specifically

11
00:00:30,060 --> 00:00:32,520
a particular kind of
deep learning models

12
00:00:32,520 --> 00:00:35,310
that have revolutionized
multiple fields,

13
00:00:35,310 --> 00:00:38,220
starting from natural
language processing

14
00:00:38,220 --> 00:00:40,470
to things like computer
vision and reinforcement

15
00:00:40,470 --> 00:00:42,930
learning to name a few.

16
00:00:42,930 --> 00:00:45,940
We have an exciting set of
videos lined up for you.

17
00:00:45,940 --> 00:00:48,090
We had some truly
fantastic speakers

18
00:00:48,090 --> 00:00:51,120
come and give talks about how
they were applying Transformers

19
00:00:51,120 --> 00:00:53,160
in their own research.

20
00:00:53,160 --> 00:00:56,430
And we hope you will enjoy
and learn from these talks.

21
00:00:56,430 --> 00:00:59,400
This video is purely
an introductory lecture

22
00:00:59,400 --> 00:01:01,530
to talk a little bit
about transformers.

23
00:01:01,530 --> 00:01:03,420
And before we get
started, I'd like

24
00:01:03,420 --> 00:01:05,370
to introduce the instructors.

25
00:01:05,370 --> 00:01:06,810
So my name is Advay.

26
00:01:06,810 --> 00:01:08,760
I am a software
engineer at a company

27
00:01:08,760 --> 00:01:10,230
called Applied Intuition.

28
00:01:10,230 --> 00:01:14,160
Before this, I was a master's
student in CS at Stanford.

29
00:01:14,160 --> 00:01:19,560
And I am one of the
co-instructors for CS25.

30
00:01:19,560 --> 00:01:22,480
Chetanya, Div, if the two of
you could introduce yourselves.

31
00:01:22,480 --> 00:01:23,790
So hi, everyone.

32
00:01:23,790 --> 00:01:26,280
I am a PhD student at Stanford.

33
00:01:26,280 --> 00:01:29,688
Before this, I was
pursuing a master's here,

34
00:01:29,688 --> 00:01:31,980
researching a lot in generative
modeling, reinforcement

35
00:01:31,980 --> 00:01:34,030
learning, and robotics.

36
00:01:34,030 --> 00:01:35,220
So nice to meet you all.

37
00:01:35,220 --> 00:01:38,400
Yeah, that was Div, since
he didn't say his name.

38
00:01:38,400 --> 00:01:40,570
Chetanya, if you want
to introduce yourself.

39
00:01:40,570 --> 00:01:41,145
Yeah.

40
00:01:41,145 --> 00:01:41,950
Hi, everyone.

41
00:01:41,950 --> 00:01:44,070
My name is Chetanya,
and I'm currently

42
00:01:44,070 --> 00:01:48,240
working as an ML engineer at
a start-up called Moveworks.

43
00:01:48,240 --> 00:01:50,130
Before that, I was
a master's student

44
00:01:50,130 --> 00:01:52,500
at Stanford specializing
in NLP and was

45
00:01:52,500 --> 00:01:54,900
a member of the
prize-winning Stanford's team

46
00:01:54,900 --> 00:01:58,180
for the Alexa Prize Challenge.

47
00:01:58,180 --> 00:01:59,580
All right, awesome.

48
00:01:59,580 --> 00:02:04,440
So moving on to the
rest of this talk,

49
00:02:04,440 --> 00:02:07,230
essentially, what we
hope you will learn

50
00:02:07,230 --> 00:02:09,180
watching these
videos and what we

51
00:02:09,180 --> 00:02:12,960
hope the people who took our
class in the fall of 2021

52
00:02:12,960 --> 00:02:15,220
learned is three things.

53
00:02:15,220 --> 00:02:17,820
One is we hope you will
have an understanding of how

54
00:02:17,820 --> 00:02:19,780
Transformers work.

55
00:02:19,780 --> 00:02:24,300
Secondly, we hope you will learn
and, by the end of these talks,

56
00:02:24,300 --> 00:02:27,630
understand how Transformers
are being applied beyond just

57
00:02:27,630 --> 00:02:29,280
natural language processing.

58
00:02:29,280 --> 00:02:32,340
And thirdly, we hope
that some of these talks

59
00:02:32,340 --> 00:02:35,428
will spark some new
ideas within you

60
00:02:35,428 --> 00:02:37,470
and hopefully lead to new
directions of research,

61
00:02:37,470 --> 00:02:40,050
new kinds of innovation,
and things of that sort.

62
00:02:40,050 --> 00:02:44,110


63
00:02:44,110 --> 00:02:47,750
And to begin, we're going
to talk a little bit

64
00:02:47,750 --> 00:02:49,700
about Transformers
and introduce some

65
00:02:49,700 --> 00:02:52,115
of the context behind
transformers as well.

66
00:02:52,115 --> 00:02:53,990
And for that, I'd like
to hand it off to Div.

67
00:02:53,990 --> 00:03:00,110


68
00:03:00,110 --> 00:03:02,070
So hi, everyone.

69
00:03:02,070 --> 00:03:05,030
So welcome to our
Transformer seminar.

70
00:03:05,030 --> 00:03:07,760
So let's start first with
an overview of the attention

71
00:03:07,760 --> 00:03:10,407
timeline and how it came to be.

72
00:03:10,407 --> 00:03:12,740
The key idea about Transformers
was the simple attention

73
00:03:12,740 --> 00:03:15,200
mechanism that was
developed in 2017.

74
00:03:15,200 --> 00:03:17,030
And this all started
with this one paper

75
00:03:17,030 --> 00:03:19,910
called "Attention is All
you Need," by Vaswani et al.

76
00:03:19,910 --> 00:03:22,490
Before 2017, we used to have
this prehistoric era where

77
00:03:22,490 --> 00:03:27,680
we had older models like RNNs,
LSTMs, and simpler attention

78
00:03:27,680 --> 00:03:28,400
mechanisms

79
00:03:28,400 --> 00:03:31,028
And eventually, the
growth in Transformers

80
00:03:31,028 --> 00:03:33,320
has exploded into other fields
and has become prominent

81
00:03:33,320 --> 00:03:35,300
in all of machine learning.

82
00:03:35,300 --> 00:03:39,810
And I'll go and see and
show how this has been used.

83
00:03:39,810 --> 00:03:43,670
So in the prehistoric era,
there used to be RNNs.

84
00:03:43,670 --> 00:03:46,520
There were different models like
the Sequence2Sequence, LSTMs,

85
00:03:46,520 --> 00:03:47,720
GRUs.

86
00:03:47,720 --> 00:03:50,210
They were good at encoding
some sort of memory,

87
00:03:50,210 --> 00:03:53,090
but they did not work for
encoding long sequences.

88
00:03:53,090 --> 00:03:55,110
And they were very bad
at encoding context.

89
00:03:55,110 --> 00:03:57,110
So here is an example.

90
00:03:57,110 --> 00:03:59,330
If you have a sentence like
I grew up in France dot,

91
00:03:59,330 --> 00:04:02,510
dot, dot, so I
speak fluent- then

92
00:04:02,510 --> 00:04:04,880
you want to fill this with
French based on the context,

93
00:04:04,880 --> 00:04:07,910
but like a LSTM model might not
know what it is and might just

94
00:04:07,910 --> 00:04:09,660
make a very big mistake here.

95
00:04:09,660 --> 00:04:13,040
Similarly, we can show some
sort of correlation map

96
00:04:13,040 --> 00:04:15,110
here, where if you
have a pronoun like it,

97
00:04:15,110 --> 00:04:17,990
we want it to correlate
to one of the past nouns

98
00:04:17,990 --> 00:04:20,720
that we have seen
so far, like animal.

99
00:04:20,720 --> 00:04:23,960
But again, older models
were really not good

100
00:04:23,960 --> 00:04:25,970
at this context encoding.

101
00:04:25,970 --> 00:04:29,620
So where we are currently now
is on the verge of take-off.

102
00:04:29,620 --> 00:04:31,730
We began to realize the
potential of Transformers

103
00:04:31,730 --> 00:04:32,880
in different fields.

104
00:04:32,880 --> 00:04:35,360
We have started to use
them to solve long sequence

105
00:04:35,360 --> 00:04:39,230
problems in protein folding,
such as the AlphaFold

106
00:04:39,230 --> 00:04:44,360
model from DeepMind,
which gets 95% accuracy

107
00:04:44,360 --> 00:04:47,660
on different challenges,
and offline RL.

108
00:04:47,660 --> 00:04:50,030
We can use it for few-shot
and zero-shot generalization,

109
00:04:50,030 --> 00:04:51,800
for text and image generation.

110
00:04:51,800 --> 00:04:53,870
And we can also use
for content generation.

111
00:04:53,870 --> 00:04:55,940
So here's an
example from OpenAI,

112
00:04:55,940 --> 00:04:58,340
where you can give a
different text prompt

113
00:04:58,340 --> 00:05:02,880
and have an AI generate a
fictional image for you.

114
00:05:02,880 --> 00:05:05,875
And so there's a doc on
this that you can also

115
00:05:05,875 --> 00:05:07,250
watch on YouTube,
which basically

116
00:05:07,250 --> 00:05:11,620
says that LSTMs are dead,
and long live Transformers.

117
00:05:11,620 --> 00:05:13,310
So what's the future?

118
00:05:13,310 --> 00:05:17,650
So we can enable a lot more
applications for Transformers.

119
00:05:17,650 --> 00:05:20,530
They can be applied to any
form of sequence modeling.

120
00:05:20,530 --> 00:05:23,290
So we could use them
for video understanding.

121
00:05:23,290 --> 00:05:25,610
We can use them for
finance and a lot more.

122
00:05:25,610 --> 00:05:28,030
So basically, imagine all
sorts of generative modeling

123
00:05:28,030 --> 00:05:29,080
problems.

124
00:05:29,080 --> 00:05:31,450
Nevertheless, there are a
lot of missing ingredients.

125
00:05:31,450 --> 00:05:33,580
So like the human
brain, we need some sort

126
00:05:33,580 --> 00:05:37,600
of external memory unit, which
is the hippocampus for us.

127
00:05:37,600 --> 00:05:40,480
And there are some
early works here.

128
00:05:40,480 --> 00:05:42,430
So one nice work you
might want to check out

129
00:05:42,430 --> 00:05:44,260
is called Neural
Turing Machines.

130
00:05:44,260 --> 00:05:46,600
Similarly, the current
attention mechanisms

131
00:05:46,600 --> 00:05:49,540
are very computationally
complex in terms

132
00:05:49,540 --> 00:05:52,490
of time and correlating
which we will discuss later.

133
00:05:52,490 --> 00:05:54,538
And we want to make
them more linear.

134
00:05:54,538 --> 00:05:56,080
And the third problem
is that we want

135
00:05:56,080 --> 00:05:58,390
to align our current
sort of language models

136
00:05:58,390 --> 00:06:01,150
with how the human brain
works and human values.

137
00:06:01,150 --> 00:06:03,220
And this is also a big issue.

138
00:06:03,220 --> 00:06:03,760
OK.

139
00:06:03,760 --> 00:06:06,490
So now I will deep dive--

140
00:06:06,490 --> 00:06:10,150
I will dive deeper into
the attention mechanisms

141
00:06:10,150 --> 00:06:13,240
and show how they
came out to be.

142
00:06:13,240 --> 00:06:18,070
So initially, they used to
be very simple mechanisms.

143
00:06:18,070 --> 00:06:20,560
Attention was inspired by
the process of importance

144
00:06:20,560 --> 00:06:24,795
weighting, of putting attention
on different parts of an image

145
00:06:24,795 --> 00:06:26,920
like similar to a human,
where you might focus more

146
00:06:26,920 --> 00:06:29,560
on a foreground if you have
an image of a dog compared

147
00:06:29,560 --> 00:06:31,070
to the rest of the background.

148
00:06:31,070 --> 00:06:33,220
So in the case of soft
attention, what you do is

149
00:06:33,220 --> 00:06:35,650
you learn the simple
soft attention weighting

150
00:06:35,650 --> 00:06:39,130
for each pixel, which can
be a weight between 0 to 1.

151
00:06:39,130 --> 00:06:40,900
The problem over
here is that this is

152
00:06:40,900 --> 00:06:42,220
a very expensive computation.

153
00:06:42,220 --> 00:06:43,900
And you can show--

154
00:06:43,900 --> 00:06:46,195
as is shown in the
figure on the left,

155
00:06:46,195 --> 00:06:48,070
you can see we are
calculating this attention

156
00:06:48,070 --> 00:06:50,230
map for the whole image.

157
00:06:50,230 --> 00:06:54,620
What you can do instead is
you can just get a 0 to 1

158
00:06:54,620 --> 00:06:57,880
attention map, where we directly
put a 1 on wherever the dog is

159
00:06:57,880 --> 00:07:00,820
and 0 wherever
it's a background.

160
00:07:00,820 --> 00:07:03,273
This is like less
computationally expensive,

161
00:07:03,273 --> 00:07:05,440
but the problem is it's
non-differentiable and makes

162
00:07:05,440 --> 00:07:07,510
things harder to train.

163
00:07:07,510 --> 00:07:09,880
Going forward, we also
have different varieties

164
00:07:09,880 --> 00:07:12,010
of basic attention
mechanisms that were

165
00:07:12,010 --> 00:07:14,170
proposed before self-attention.

166
00:07:14,170 --> 00:07:17,350
So the first term right here
is global attention models.

167
00:07:17,350 --> 00:07:22,243
So global attention model
is for each hidden layer

168
00:07:22,243 --> 00:07:23,410
input-- hidden layer output.

169
00:07:23,410 --> 00:07:26,230
You learn attention
weight of a of p

170
00:07:26,230 --> 00:07:28,330
and this is
elementwise multiplied

171
00:07:28,330 --> 00:07:32,650
with your current output to
get your final output, yt.

172
00:07:32,650 --> 00:07:35,290
Similarly, you have the
local attention models,

173
00:07:35,290 --> 00:07:37,480
where instead of calculating
the global attention

174
00:07:37,480 --> 00:07:39,850
for over the whole
sequence length,

175
00:07:39,850 --> 00:07:44,220
you only calculate the
attention over a small window.

176
00:07:44,220 --> 00:07:47,260
And then you weight by the
attention of the window

177
00:07:47,260 --> 00:07:52,420
into the current output to
get the final output you need.

178
00:07:52,420 --> 00:07:55,030
So moving on, I will
pass on to Chetanya

179
00:07:55,030 --> 00:07:59,290
to discuss self-attention
mechanisms and platforms.

180
00:07:59,290 --> 00:08:02,280
Thank you, Div, for covering
a brief overview of how

181
00:08:02,280 --> 00:08:05,220
the primitive versions
of attention work.

182
00:08:05,220 --> 00:08:09,000
Now just before we talk about
self-attention, just a bit

183
00:08:09,000 --> 00:08:13,230
of trivia, that term was
first introduced by a paper

184
00:08:13,230 --> 00:08:16,440
from Lin et al., which
provided a framework

185
00:08:16,440 --> 00:08:22,440
for a self-attentive mechanism
for sentence and meanings.

186
00:08:22,440 --> 00:08:25,920
And now moving on to the
main crux of the Transformers

187
00:08:25,920 --> 00:08:28,230
paper, which was the
self-attention block.

188
00:08:28,230 --> 00:08:32,370
So self-attention is the
basis, is the main building

189
00:08:32,370 --> 00:08:36,450
block for what makes a
Transformers model work so well

190
00:08:36,450 --> 00:08:40,179
and to enable them and
make them so powerful.

191
00:08:40,179 --> 00:08:42,630
So to think of it
more easily, we

192
00:08:42,630 --> 00:08:44,970
can break down
the self-attention

193
00:08:44,970 --> 00:08:46,870
as a search retrieval problem.

194
00:08:46,870 --> 00:08:51,010
So the problem is that
given a query, q, and v,

195
00:08:51,010 --> 00:08:53,070
we need to find
a set of keys, k,

196
00:08:53,070 --> 00:08:55,230
which are most similar
to q and return

197
00:08:55,230 --> 00:08:58,530
the corresponding key
values called v. Now

198
00:08:58,530 --> 00:09:00,990
these three vectors can be
drawn from the same source.

199
00:09:00,990 --> 00:09:03,690
For example, we can
have that q, k, and v

200
00:09:03,690 --> 00:09:06,360
are all equal to a single
vector x, where x can

201
00:09:06,360 --> 00:09:08,460
be output of a previous layer.

202
00:09:08,460 --> 00:09:11,100
In Transformers,
these vectors are

203
00:09:11,100 --> 00:09:14,220
obtained by applying different
linear transformations to x.

204
00:09:14,220 --> 00:09:17,070
So as to enable the
model to capture

205
00:09:17,070 --> 00:09:20,550
more complex interactions
between the different tokens

206
00:09:20,550 --> 00:09:22,780
at different places
of the sentence.

207
00:09:22,780 --> 00:09:26,190
Now how attention is computed
is just a weighted summation

208
00:09:26,190 --> 00:09:29,950
of the similarities between
the query and key vectors,

209
00:09:29,950 --> 00:09:31,680
which is weighted by
the respective value

210
00:09:31,680 --> 00:09:33,120
for those keys.

211
00:09:33,120 --> 00:09:35,580
And in the Transformers
paper, they

212
00:09:35,580 --> 00:09:38,160
used the scaled dot-product
as a similarity function

213
00:09:38,160 --> 00:09:40,110
for the queries and keys.

214
00:09:40,110 --> 00:09:42,540
And another important
aspect of the Transformers

215
00:09:42,540 --> 00:09:46,200
was the introduction of
Multi-head self-attention.

216
00:09:46,200 --> 00:09:48,150
So what Multi-head
Self-attention means

217
00:09:48,150 --> 00:09:51,600
is that the self-attention
is for at every layer

218
00:09:51,600 --> 00:09:54,090
the self-attention is
performed multiple times,

219
00:09:54,090 --> 00:09:57,270
which enables the model to
learn multiple representations

220
00:09:57,270 --> 00:09:58,600
of spaces.

221
00:09:58,600 --> 00:10:05,280
So in a way, you can think of
it that each head has a power

222
00:10:05,280 --> 00:10:08,460
to look at different things and
to learn different semantics.

223
00:10:08,460 --> 00:10:11,460
For example, one
head can be learning

224
00:10:11,460 --> 00:10:14,460
to try to predict what
is the part of speech

225
00:10:14,460 --> 00:10:15,240
for those tokens.

226
00:10:15,240 --> 00:10:18,720
One head might be learning
what is the syntactic structure

227
00:10:18,720 --> 00:10:21,300
of the sentence and
all those things

228
00:10:21,300 --> 00:10:26,400
that are there to
understand what the upcoming

229
00:10:26,400 --> 00:10:28,800
sentence means.

230
00:10:28,800 --> 00:10:31,303
Now to better understand
what the self-attention works

231
00:10:31,303 --> 00:10:32,970
and what are the
different computations,

232
00:10:32,970 --> 00:10:35,200
there is a short video.

233
00:10:35,200 --> 00:10:39,330
So in this-- so as
you can see, there

234
00:10:39,330 --> 00:10:43,260
are three incoming tokens,
so input 1, input 2, input 3.

235
00:10:43,260 --> 00:10:47,820
We apply linear transformations
to get the key value vectors

236
00:10:47,820 --> 00:10:51,600
for each input and then
give-- once a query, q, comes,

237
00:10:51,600 --> 00:10:55,050
we calculate its similarity
with the respective key vectors

238
00:10:55,050 --> 00:10:59,430
and then multiply those
scores with the value vector.

239
00:10:59,430 --> 00:11:02,730
And then add them all
up to get the output.

240
00:11:02,730 --> 00:11:06,510
The same computation is then
performed on all the tokens.

241
00:11:06,510 --> 00:11:10,350
And we get the output of
the self-attention layer.

242
00:11:10,350 --> 00:11:12,690
So as you can see
here, the final output

243
00:11:12,690 --> 00:11:15,150
of self-attention layer
is in dark green that's

244
00:11:15,150 --> 00:11:17,750
at the top of the screen.

245
00:11:17,750 --> 00:11:19,480
So now again, for
the final token,

246
00:11:19,480 --> 00:11:22,810
we perform everything same,
queries multiplied by keys.

247
00:11:22,810 --> 00:11:24,460
We get the similarity scores.

248
00:11:24,460 --> 00:11:27,790
And then those similarity
scores weigh the value vectors.

249
00:11:27,790 --> 00:11:29,590
And then we finally
perform the addition

250
00:11:29,590 --> 00:11:34,090
to get the self-attention output
of the Transformers block.

251
00:11:34,090 --> 00:11:39,240


252
00:11:39,240 --> 00:11:41,460
Apart from
self-attention, there are

253
00:11:41,460 --> 00:11:44,640
some other necessary
ingredients that makes

254
00:11:44,640 --> 00:11:46,470
the Transformers so powerful.

255
00:11:46,470 --> 00:11:49,170
One important aspect
is the presence

256
00:11:49,170 --> 00:11:51,630
of positional representations
or the embedding layer.

257
00:11:51,630 --> 00:11:56,280
So the way RNNs
worked very well was

258
00:11:56,280 --> 00:11:58,710
that since they process
each of the information

259
00:11:58,710 --> 00:12:00,120
in a sequential ordering.

260
00:12:00,120 --> 00:12:03,630
So there was this notion
of ordering, right,

261
00:12:03,630 --> 00:12:06,180
which is also very important
in understanding language

262
00:12:06,180 --> 00:12:10,020
because we all know that
we read any piece of text

263
00:12:10,020 --> 00:12:14,970
from left to right in
most of the languages

264
00:12:14,970 --> 00:12:17,200
and also right to left
in some languages.

265
00:12:17,200 --> 00:12:19,080
So there is a notion
of ordering which

266
00:12:19,080 --> 00:12:20,790
is lost in kind
of self-attention

267
00:12:20,790 --> 00:12:24,060
because every word is
attending to every other word.

268
00:12:24,060 --> 00:12:27,900
That's why this paper
introduced a separate embedding

269
00:12:27,900 --> 00:12:30,990
layer for introducing
positional representations.

270
00:12:30,990 --> 00:12:34,230
The second important aspect
is having nonlinearities.

271
00:12:34,230 --> 00:12:36,822
So if you think of all
the computation that

272
00:12:36,822 --> 00:12:38,530
is happening in the
self-attention layer,

273
00:12:38,530 --> 00:12:41,130
it's all linear because it's
all matrix multiplication.

274
00:12:41,130 --> 00:12:44,190
But as we all know, that
deep learning models

275
00:12:44,190 --> 00:12:46,980
work well when
they are able to--

276
00:12:46,980 --> 00:12:50,070
when they are able to learn more
complex mappings between input

277
00:12:50,070 --> 00:12:54,270
and output, which can be
attained by a simple MLP.

278
00:12:54,270 --> 00:12:56,640
And the third important
component of the self--

279
00:12:56,640 --> 00:12:59,020
of the Transformers
is the masking.

280
00:12:59,020 --> 00:13:03,060
So masking is what allows to
parallelize the operations.

281
00:13:03,060 --> 00:13:05,490
Since every word can
attend to every other word,

282
00:13:05,490 --> 00:13:08,392
in the decoder part
of our transformers,

283
00:13:08,392 --> 00:13:10,350
which Advay is going to
be talking about later,

284
00:13:10,350 --> 00:13:13,800
is the problem becomes that
you don't want the decoder

285
00:13:13,800 --> 00:13:15,840
to look into the
future because that

286
00:13:15,840 --> 00:13:18,210
can result in data leakage.

287
00:13:18,210 --> 00:13:20,730
So that's why masking
helps the decoder

288
00:13:20,730 --> 00:13:26,805
to avoid that future information
and learn only what has been--

289
00:13:26,805 --> 00:13:29,590
what the model has
processed so far.

290
00:13:29,590 --> 00:13:33,450
So now onto the
encoder-decoder architecture

291
00:13:33,450 --> 00:13:34,410
of the Transformers.

292
00:13:34,410 --> 00:13:36,310
Advay.

293
00:13:36,310 --> 00:13:36,810
Yeah.

294
00:13:36,810 --> 00:13:39,730
Thanks, Chetanya, for
talking about self-attention.

295
00:13:39,730 --> 00:13:44,130
So self-attention is sort
of the key ingredient or one

296
00:13:44,130 --> 00:13:46,200
of the key ingredients
that allows Transformers

297
00:13:46,200 --> 00:13:48,910
to work so well, but
at a very high level,

298
00:13:48,910 --> 00:13:52,020
the model that was proposed
in the Vaswani et al.

299
00:13:52,020 --> 00:13:56,670
paper of 2017 was like previous
language models in the sense

300
00:13:56,670 --> 00:13:59,520
that it had an
encoder-decoder architecture.

301
00:13:59,520 --> 00:14:00,857
What that means is--

302
00:14:00,857 --> 00:14:02,940
let's say you're working
on a translation problem.

303
00:14:02,940 --> 00:14:04,800
You want to translate
English to French.

304
00:14:04,800 --> 00:14:06,300
The way that would
work is you would

305
00:14:06,300 --> 00:14:10,140
read in the entire input
of your English sentence.

306
00:14:10,140 --> 00:14:11,770
You would encode that input.

307
00:14:11,770 --> 00:14:13,870
So that's the encoded
part of the network.

308
00:14:13,870 --> 00:14:16,020
And then you would
generate token

309
00:14:16,020 --> 00:14:18,420
by token the corresponding
French translation.

310
00:14:18,420 --> 00:14:20,580
And the decoder is the
part of the network

311
00:14:20,580 --> 00:14:24,430
that is responsible for
generating those tokens.

312
00:14:24,430 --> 00:14:27,990
So you can think of these
encoder blocks and decoder

313
00:14:27,990 --> 00:14:30,990
blocks as essentially
something like LEGO.

314
00:14:30,990 --> 00:14:34,560
They have these subcomponents
that make them up.

315
00:14:34,560 --> 00:14:36,600
And in particular,
the encoder block

316
00:14:36,600 --> 00:14:38,680
has three main subcomponents.

317
00:14:38,680 --> 00:14:40,860
The first is the
self-attention layer

318
00:14:40,860 --> 00:14:43,140
that Chetanya talked
about earlier.

319
00:14:43,140 --> 00:14:46,180
And, as talked about
earlier as well,

320
00:14:46,180 --> 00:14:48,510
you need a feed-forward
layer after that

321
00:14:48,510 --> 00:14:50,460
because the
self-attention layer only

322
00:14:50,460 --> 00:14:52,140
performs linear operations.

323
00:14:52,140 --> 00:14:55,560
And so you need something that
can capture the nonlinearities.

324
00:14:55,560 --> 00:14:58,120
You also have a layer
norm after this.

325
00:14:58,120 --> 00:15:00,300
And lastly, there are
residual connections

326
00:15:00,300 --> 00:15:02,820
between different
encoder blocks.

327
00:15:02,820 --> 00:15:05,195
The decoder is very
similar to the encoder,

328
00:15:05,195 --> 00:15:06,570
but there's one
difference, which

329
00:15:06,570 --> 00:15:09,720
is that it has this extra layer,
because the decoder doesn't

330
00:15:09,720 --> 00:15:12,600
just do Multi-head
Attention on the output

331
00:15:12,600 --> 00:15:14,250
of the previous layers.

332
00:15:14,250 --> 00:15:17,425
So for context, the encoder
does Multi-head Attention

333
00:15:17,425 --> 00:15:21,360
for each self-attention
layer in the encoder block.

334
00:15:21,360 --> 00:15:25,020
And each of the encoder blocks
does Multi-head Attention

335
00:15:25,020 --> 00:15:29,550
looking at the previous
layers of the encoder blocks.

336
00:15:29,550 --> 00:15:33,000
The decoder, however,
does that in the sense

337
00:15:33,000 --> 00:15:35,880
that it also looks at the
previous layers of the decoder,

338
00:15:35,880 --> 00:15:38,460
but it also looks at the
output of the encoder.

339
00:15:38,460 --> 00:15:40,590
And so it needs a
Multi-head Attention layer

340
00:15:40,590 --> 00:15:43,910
over the encoder blocks.

341
00:15:43,910 --> 00:15:46,860
And lastly, there's
masking as well.

342
00:15:46,860 --> 00:15:49,520
So if you are--
because every token can

343
00:15:49,520 --> 00:15:51,650
look at every other
token, you want

344
00:15:51,650 --> 00:15:53,480
to sort of make
sure in the decoder

345
00:15:53,480 --> 00:15:55,260
that you're not looking
into the future.

346
00:15:55,260 --> 00:15:57,380
So if you're in position
3, for instance,

347
00:15:57,380 --> 00:15:59,960
you shouldn't be able to look
at position 4 and position 5.

348
00:15:59,960 --> 00:16:03,120


349
00:16:03,120 --> 00:16:05,190
So those are sort of
all the components

350
00:16:05,190 --> 00:16:09,750
that led to the creation of
the model in the Vaswani et al.

351
00:16:09,750 --> 00:16:10,770
paper.

352
00:16:10,770 --> 00:16:14,310
And let's talk a little
bit about the advantages

353
00:16:14,310 --> 00:16:16,630
and drawbacks of this model.

354
00:16:16,630 --> 00:16:20,040
So the two main advantages which
are huge advantages and which

355
00:16:20,040 --> 00:16:22,980
are why Transformers
have done such a good job

356
00:16:22,980 --> 00:16:28,560
of revolutionizing many, many
fields within deep learning

357
00:16:28,560 --> 00:16:29,970
are as follows.

358
00:16:29,970 --> 00:16:32,235
So the first is there
is this constant path

359
00:16:32,235 --> 00:16:35,280
length between any two
positions in a sequence

360
00:16:35,280 --> 00:16:37,890
because every token
in the sequence

361
00:16:37,890 --> 00:16:39,780
is looking at every other token.

362
00:16:39,780 --> 00:16:41,850
And this basically
solves the problem

363
00:16:41,850 --> 00:16:44,580
that they've talked about
earlier with long sequences.

364
00:16:44,580 --> 00:16:46,740
You don't have this
problem with long sequences

365
00:16:46,740 --> 00:16:49,110
where if you're trying
to predict a token

366
00:16:49,110 --> 00:16:53,867
that depends on a word that was
far, far behind in a sentence.

367
00:16:53,867 --> 00:16:55,950
You don't have the problem
of losing that context.

368
00:16:55,950 --> 00:16:58,440
Now the distance
between them is only one

369
00:16:58,440 --> 00:17:00,690
in terms of the path length.

370
00:17:00,690 --> 00:17:03,120
Also, because of the nature
of the computation that's

371
00:17:03,120 --> 00:17:05,730
happening, Transformer
models lend themselves really

372
00:17:05,730 --> 00:17:07,770
well to parallelization
and because

373
00:17:07,770 --> 00:17:09,990
of the advances that
we've had with GPUs.

374
00:17:09,990 --> 00:17:12,210
Basically, if you take
a Transformer model

375
00:17:12,210 --> 00:17:15,358
with n parameters and you take a
model that isn't a Transformer,

376
00:17:15,358 --> 00:17:18,064
say, like an LSTM also
with n parameters,

377
00:17:18,065 --> 00:17:19,440
training the
Transformer model is

378
00:17:19,440 --> 00:17:22,829
going to be much faster
because of the parallelization

379
00:17:22,829 --> 00:17:24,329
that it leverages.

380
00:17:24,329 --> 00:17:26,170
So those are the advantages.

381
00:17:26,170 --> 00:17:30,360
The disadvantages are
basically self-attention takes

382
00:17:30,360 --> 00:17:32,310
quadratic time because
every token looks

383
00:17:32,310 --> 00:17:33,540
at every other token.

384
00:17:33,540 --> 00:17:36,300
Order n squared as you
might know does not scale,

385
00:17:36,300 --> 00:17:38,250
and there's actually
been a lot of work

386
00:17:38,250 --> 00:17:40,030
in trying to tackle this.

387
00:17:40,030 --> 00:17:41,370
So we've linked to some here.

388
00:17:41,370 --> 00:17:43,200
Big Bird, Linformer,
and Reformer

389
00:17:43,200 --> 00:17:47,040
are all approaches to try and
make this linear or quasilinear

390
00:17:47,040 --> 00:17:48,190
essentially.

391
00:17:48,190 --> 00:17:51,130


392
00:17:51,130 --> 00:17:53,920
And yeah, we highly
recommend to--

393
00:17:53,920 --> 00:17:55,780
recommend going through
Jay Alammar's blog,

394
00:17:55,780 --> 00:17:57,610
"The Illustrated
Transformer." which

395
00:17:57,610 --> 00:18:00,670
provides great visualizations
and explains everything

396
00:18:00,670 --> 00:18:04,490
that we just talked
about in great detail.

397
00:18:04,490 --> 00:18:04,990
Yeah.

398
00:18:04,990 --> 00:18:07,950
And I'd like to pass it on
to Chetanya for applications

399
00:18:07,950 --> 00:18:10,150
of Transformers.

400
00:18:10,150 --> 00:18:10,650
Yeah.

401
00:18:10,650 --> 00:18:13,560
So now moving on to some
of the recent work--

402
00:18:13,560 --> 00:18:16,380
some of the work that
very shortly followed

403
00:18:16,380 --> 00:18:18,190
the Transformers paper.

404
00:18:18,190 --> 00:18:21,300
So one of the
models that came out

405
00:18:21,300 --> 00:18:25,880
was GPT, the GPT architecture,
which was released by OpenAI.

406
00:18:25,880 --> 00:18:29,700
So OpenAI had the latest model
that OpenAI has and the GPT

407
00:18:29,700 --> 00:18:31,380
series and the GPT-3.

408
00:18:31,380 --> 00:18:33,630
So it consists of
only the decoder

409
00:18:33,630 --> 00:18:34,980
blocks from Transformers.

410
00:18:34,980 --> 00:18:37,140
And it's trained on a
traditional language modeling

411
00:18:37,140 --> 00:18:40,650
task, which is predicting
the current token-- which

412
00:18:40,650 --> 00:18:44,930
is creating the next token
given the last t tokens

413
00:18:44,930 --> 00:18:46,650
that the model has seen.

414
00:18:46,650 --> 00:18:49,440
And for any downstream
tasks, now the model

415
00:18:49,440 --> 00:18:52,200
can just-- you can just
train a classification layer

416
00:18:52,200 --> 00:18:57,630
on the last hidden state, which
can have any number of labels.

417
00:18:57,630 --> 00:19:01,870
And since the model is
generative in nature,

418
00:19:01,870 --> 00:19:04,950
you can also use the
pretrained network

419
00:19:04,950 --> 00:19:09,120
for generative kind of
tasks, such as summarization

420
00:19:09,120 --> 00:19:13,440
and natural language
generation for that instance.

421
00:19:13,440 --> 00:19:17,340
Another important aspect
that GPT gained popularity

422
00:19:17,340 --> 00:19:20,430
was its ability to
be able to perform

423
00:19:20,430 --> 00:19:22,380
in-context learning,
what the authors called

424
00:19:22,380 --> 00:19:23,680
in-context learning.

425
00:19:23,680 --> 00:19:27,360
So this is the ability
wherein the model can

426
00:19:27,360 --> 00:19:30,210
learn under few-shot
settings, what

427
00:19:30,210 --> 00:19:32,460
the task is to complete
the task without performing

428
00:19:32,460 --> 00:19:33,690
any gradient updates.

429
00:19:33,690 --> 00:19:35,460
For example, let's
say, the model

430
00:19:35,460 --> 00:19:38,770
has shown a bunch of
addition examples.

431
00:19:38,770 --> 00:19:43,710
And then if you pass in a
new input and leave the--

432
00:19:43,710 --> 00:19:46,650
and just leave it
at equal to sign,

433
00:19:46,650 --> 00:19:50,340
the model tries to predict
the next token, which

434
00:19:50,340 --> 00:19:55,080
very well comes out to be
the sum of the numbers that

435
00:19:55,080 --> 00:19:55,800
is shown.

436
00:19:55,800 --> 00:19:58,590
Another example can be
also the spell correction

437
00:19:58,590 --> 00:20:00,540
task or the translation task.

438
00:20:00,540 --> 00:20:06,870
So this was the ability that
made GPT-3 so much talked about

439
00:20:06,870 --> 00:20:08,140
in the NLP world.

440
00:20:08,140 --> 00:20:11,520
And right now also,
many applications

441
00:20:11,520 --> 00:20:15,930
have been made using GPT-3
which includes one of them being

442
00:20:15,930 --> 00:20:21,930
the VS Code Copilot, which tries
to generate a piece of code

443
00:20:21,930 --> 00:20:26,310
given docstring kind of
natural language text.

444
00:20:26,310 --> 00:20:29,370
Another major
model that came out

445
00:20:29,370 --> 00:20:31,380
that was based on the
Transformers' architecture

446
00:20:31,380 --> 00:20:32,280
was BERT.

447
00:20:32,280 --> 00:20:35,010
So BERT lends its
name from-- it's

448
00:20:35,010 --> 00:20:38,340
an acronym for Bidirectional
Encoder Representations

449
00:20:38,340 --> 00:20:39,630
from Transformers.

450
00:20:39,630 --> 00:20:43,350
It consists of only the encoder
blocks of the Transformers,

451
00:20:43,350 --> 00:20:48,450
which is unlike GPT-3, which
had only the decoder blocks.

452
00:20:48,450 --> 00:20:52,710
Because of this change,
there comes a problem

453
00:20:52,710 --> 00:20:55,210
because BERT has only
the encoder block.

454
00:20:55,210 --> 00:20:57,300
So it sees the
entire piece of text.

455
00:20:57,300 --> 00:21:00,300
It cannot be pretrained on a
naive language modeling task

456
00:21:00,300 --> 00:21:03,700
because of the problem of
data leakage from the future.

457
00:21:03,700 --> 00:21:08,010
So what the authors came
up with was a clever idea.

458
00:21:08,010 --> 00:21:10,590
And they came up with a novel
task called Masked Language

459
00:21:10,590 --> 00:21:14,340
Modeling, which included
to replace certain words

460
00:21:14,340 --> 00:21:15,210
with a placeholder.

461
00:21:15,210 --> 00:21:17,880
And then the model tries to
predict those words given

462
00:21:17,880 --> 00:21:20,060
the entire context.

463
00:21:20,060 --> 00:21:23,600
Now, apart from this
token-level task,

464
00:21:23,600 --> 00:21:25,790
the authors also added
a second objective

465
00:21:25,790 --> 00:21:27,540
called the Next Sentence
Prediction, which

466
00:21:27,540 --> 00:21:31,250
was a sentence-level
task, wherein

467
00:21:31,250 --> 00:21:34,490
given two chunks of text,
the model tried to predict

468
00:21:34,490 --> 00:21:37,550
whether the second sentence
followed the other sentence

469
00:21:37,550 --> 00:21:39,800
or not, followed the
first sentence or not.

470
00:21:39,800 --> 00:21:43,790
And now after pretraining this
model for any downstream tasks,

471
00:21:43,790 --> 00:21:45,260
the model can be
further fine-tuned

472
00:21:45,260 --> 00:21:46,885
with an additional
classification layer

473
00:21:46,885 --> 00:21:49,720
just like it was in GPT-3.

474
00:21:49,720 --> 00:21:54,190
So these are the two models
that have been very popular

475
00:21:54,190 --> 00:21:57,760
and have made a lot of
applications, made their way

476
00:21:57,760 --> 00:21:59,120
in a lot of applications.

477
00:21:59,120 --> 00:22:01,390
But the landscape has
changed quite a lot

478
00:22:01,390 --> 00:22:02,710
since we have taken this class.

479
00:22:02,710 --> 00:22:05,470
There are models with
different computing techniques

480
00:22:05,470 --> 00:22:07,060
like ELECTRA, DeBERTa.

481
00:22:07,060 --> 00:22:09,490
And there are also
models that do

482
00:22:09,490 --> 00:22:12,707
well in like other
modalities and which

483
00:22:12,707 --> 00:22:15,040
we are going to be talking
about in other lecture series

484
00:22:15,040 --> 00:22:16,070
as well.

485
00:22:16,070 --> 00:22:18,800
So yeah, that's all
from this lecture.

486
00:22:18,800 --> 00:22:21,670
And thank you for tuning in.

487
00:22:21,670 --> 00:22:22,840
Yeah.

488
00:22:22,840 --> 00:22:24,790
Just want to end by
saying, thank you

489
00:22:24,790 --> 00:22:25,720
all for watching this.

490
00:22:25,720 --> 00:22:28,360
And we have a
really exciting set

491
00:22:28,360 --> 00:22:31,000
of videos with truly
amazing speakers.

492
00:22:31,000 --> 00:22:33,930
And we hope you are able
to derive value from that.

493
00:22:33,930 --> 00:22:34,810
Sure.

494
00:22:34,810 --> 00:22:35,600
Thanks a lot.

495
00:22:35,600 --> 00:22:36,100
Thank you.

496
00:22:36,100 --> 00:22:38,220
Thank you, everyone.

497
00:22:38,220 --> 00:22:43,200


